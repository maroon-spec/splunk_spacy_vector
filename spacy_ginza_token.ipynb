{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Splunk Machine Learning Toolkit Container for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains an example workflow how to work on custom containerized code that seamlessly interfaces with the Splunk Machine Learning Toolkit (MLTK) Container for TensorFlow. This script contains an example of how to run an entity extraction algorithm over text using the spacy library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 - import libraries\n",
    "At stage 0 we define all imports necessary to run our subsequent code depending on various libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "name": "mltkc_import"
   },
   "outputs": [],
   "source": [
    "# this definition exposes all python module imports that should be available in all subsequent commands\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import sys\n",
    "\n",
    "# global constants\n",
    "MODEL_DIRECTORY = \"/srv/app/model/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.16.4\n",
      "pandas version: 0.25.1\n",
      "spacy version: 2.2.4\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "print(\"numpy version: \" + np.__version__)\n",
    "print(\"pandas version: \" + pd.__version__)\n",
    "print(\"spacy version: \" + spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1 - get a data sample from Splunk\n",
    "In Splunk run a search to pipe a prepared dataset into this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| inputlookup fy21_basic | rename \"本日の感想やご意見をお聞かせください\" as comment, \"本日のワークショップにご満足いただけましたか？\" as satisfy, \"ワークショップ実施日\" as date| where isnotnull('comment') | eval workshop = if(date = \"7/17/2020\",\"AWS Security\",\"Basic\") | append [| inputlookup fy21_premium | rename \"本日の感想や率直なご意見をお聞かせください\" as comment, \"本日のワークショップにご満足いただけましたか？\" as satisfy, \"ワークショップ実施日\" as date , \"本日のワークショップ種類\" as workshop| where isnotnull('comment') ] | table workshop date satisfy comment \n",
    "| fit MLTKContainer mode=stage algo=spacy_ginza_token comment from date workshop satisfy into ws_spacy_token_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you run this search your data set sample is available as a csv inside the container to develop your model. The name is taken from the into keyword (\"spacy_entity_extraction_model in the example above) or set to \"default\" if no into keyword is present. This step is intended to work with a subset of your data to create your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "name": "mltkc_stage"
   },
   "outputs": [],
   "source": [
    "# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\n",
    "def stage(name):\n",
    "    with open(\"data/\"+name+\".csv\", 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    with open(\"data/\"+name+\".json\", 'r') as f:\n",
    "        param = json.load(f)\n",
    "    return df, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  satisfy                                            comment  \\\n",
      "0    2/14/2020        4                             本格的に使うとなるとやはり難しそうではある。   \n",
      "1    2/14/2020        5                  Splunkの基本動作の勉強になりました。ありがとうございました。   \n",
      "2    2/14/2020        5  知らないコマンドを知れただけで有益でした。\\nsplunkはもっとたくさん活用できる可能性が...   \n",
      "3    2/14/2020        5                                分かりやすかったです。まず使ってみます   \n",
      "4    2/14/2020        5  事前にsplunkを少しでも触っていると飲み込みやすい内容だった。splunkの特長を強調し...   \n",
      "..         ...      ...                                                ...   \n",
      "343  8/28/2020        4              セキュリティは現在の主業務ではありませんがログ分析の方法は参考になりました   \n",
      "344  8/28/2020        4    途中参加でしたが、途中からでもログソースや各ログ内のフィールド、値がわかって参考になりました。   \n",
      "345  8/28/2020        5             実践的な事案でのハンズオンで、大変勉強になりました。ありがとうございました。   \n",
      "346  8/28/2020        4  自分にはPowerPoint内の課題の難易度が高かったですが、Office365分析における...   \n",
      "347  8/28/2020        5  他社のSIEM製品を使用した運用を実施しているが、Splunkについては未経験の中スケジュー...   \n",
      "\n",
      "           workshop  \n",
      "0             Basic  \n",
      "1             Basic  \n",
      "2             Basic  \n",
      "3             Basic  \n",
      "4             Basic  \n",
      "..              ...  \n",
      "343  Azure Security  \n",
      "344  Azure Security  \n",
      "345  Azure Security  \n",
      "346  Azure Security  \n",
      "347  Azure Security  \n",
      "\n",
      "[348 rows x 4 columns]\n",
      "(348, 4)\n",
      "{'options': {'params': {'mode': 'stage', 'algo': 'spacy_ginza_token'}, 'args': ['comment', 'date', 'workshop', 'satisfy'], 'target_variable': ['comment'], 'feature_variables': ['date', 'workshop', 'satisfy'], 'model_name': 'ws_spacy_token_stage', 'algo_name': 'MLTKContainer', 'mlspl_limits': {'handle_new_cat': 'default', 'max_distinct_cat_values': '100', 'max_distinct_cat_values_for_classifiers': '100', 'max_distinct_cat_values_for_scoring': '100', 'max_fit_time': '600', 'max_inputs': '100000', 'max_memory_usage_mb': '1000', 'max_model_size_mb': '15', 'max_score_time': '600', 'streaming_apply': 'false', 'use_sampling': 'true'}, 'kfold_cv': None}, 'feature_variables': ['date', 'workshop', 'satisfy'], 'target_variables': ['comment']}\n"
     ]
    }
   ],
   "source": [
    "# THIS CELL IS NOT EXPORTED - free notebook cell for testing purposes\n",
    "df, param = stage(\"ws_spacy_token_stage\")\n",
    "print(df)\n",
    "print(df.shape)\n",
    "print(str(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 - create and initialize a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "name": "mltkc_init"
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "# params: data and parameters\n",
    "# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n",
    "def init(df,param):\n",
    "    # Load English tokenizer, tagger, parser, NER and word vectors\n",
    "    #import en_core_web_sm\n",
    "    #model = en_core_web_sm.load()\n",
    "    #model = spacy.load(\"en_core_web_sm\")\n",
    "    model = spacy.load(\"ja_ginza\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = init(df,param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3 - fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for this algorithm the model is pre-trained (the en_core_web_sm library comes pre-packaged by spacy) and therefore this stage is a placeholder only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "name": "mltkc_fit"
   },
   "outputs": [],
   "source": [
    "# returns a fit info json object\n",
    "def fit(model,df,param):\n",
    "    returns = {}\n",
    "    \n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 4 - apply the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "name": "mltkc_apply"
   },
   "outputs": [],
   "source": [
    "def apply(model,df,param):\n",
    "    X = df['comment'].values.tolist()\n",
    "    \n",
    "    returns = list()\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        doc = model(str(X[i]))\n",
    "        \n",
    "        \n",
    "        entities = ''\n",
    "        stop_words = ['[',']','、','。','.',',','\\'','です']\n",
    "        \n",
    "        # Find named entities, phrases and concepts\n",
    "        for entity in doc:\n",
    "            if str(entities) in stop_words or len(entity) == 1:\n",
    "                      continue\n",
    "            elif entities == '':\n",
    "                entities = entities + entity.text + ':' + entity.pos_\n",
    "            else:\n",
    "                entities = entities + '|' + entity.text + ':' + entity.pos_\n",
    "        \n",
    "        returns.append(entities)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['本格的に使うとなるとやはり難しそうではある。',\n",
       " 'Splunkの基本動作の勉強になりました。ありがとうございました。',\n",
       " '知らないコマンドを知れただけで有益でした。\\nsplunkはもっとたくさん活用できる可能性があると思いますが、セミナー等が少なく、販売代理店からはスキルを学べないため、より多くの情報を収集したいです。',\n",
       " '分かりやすかったです。まず使ってみます']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comment'].values.tolist()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['本格的:NOUN|使う:VERB|なる:VERB|やはり:ADV|難し:ADJ|そう:ADJ|ある:AUX',\n",
       " 'Splunk:NOUN|基本:NOUN|動作:NOUN|勉強:NOUN|なり:VERB|まし:AUX|ありがとう:INTJ|ござい:VERB|まし:AUX',\n",
       " '知ら:VERB|ない:AUX|コマンド:NOUN|知れ:VERB|だけ:ADP|有益:ADJ|でし:AUX|splunk:NOUN|もっと:ADV|たくさん:ADV|活用:VERB|できる:VERB|可能性:NOUN|ある:VERB|思い:VERB|ます:AUX|セミナー:NOUN|少なく:ADJ|販売:NOUN|代理店:NOUN|から:ADP|スキル:NOUN|学べ:VERB|ない:AUX|ため:NOUN|より:ADV|多く:NOUN|情報:NOUN|収集:VERB|たい:AUX|です:AUX',\n",
       " '分かり:VERB|やすかっ:NOUN|です:AUX|まず:ADV|使っ:VERB|ます:AUX',\n",
       " '事前:NOUN|splunk:NOUN|少し:ADV|触っ:VERB|いる:AUX|飲み込み:VERB|やすい:NOUN|内容:NOUN|だっ:AUX|splunk:NOUN|特長:NOUN|強調:VERB|喋っ:VERB|いただい:AUX|特長:NOUN|掴み:VERB|やすかっ:NOUN']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns = apply(model,df,param)\n",
    "returns[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5 - save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "name": "mltkc_save"
   },
   "outputs": [],
   "source": [
    "# save model to name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def save(model,name):\n",
    "    # model will not be saved or reloaded as it is pre-built\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 6 - load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "name": "mltkc_load"
   },
   "outputs": [],
   "source": [
    "# load model from name in expected convention \"<algo_name>_<model_name>.h5\"\n",
    "def load(name):\n",
    "    # model will not be saved or reloaded as it is pre-built\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 7 - provide a summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "name": "mltkc_summary"
   },
   "outputs": [],
   "source": [
    "# return model summary\n",
    "def summary(model=None):\n",
    "    returns = {\"version\": {\"spacy\": spacy.__version__} }\n",
    "    if model is not None:\n",
    "        # Save keras model summary to string:\n",
    "        s = []\n",
    "        returns[\"summary\"] = ''.join(s)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Stages\n",
    "All subsequent cells are not tagged and can be used for further freeform code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
